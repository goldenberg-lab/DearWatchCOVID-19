{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Flu Onset Prediction</h1>\n",
    "Table of contents:\n",
    "<ol>\n",
    "    <li><a href=\"#imputation\">Imputation</a></li>\n",
    "    <li><a href=\"#time_to_onset\">Time to Onset</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm_pandas\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from dataclasses import dataclass, asdict\n",
    "from copy import deepcopy\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from constants import *\n",
    "from run_model import apply_limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace argparse args in notebook\n",
    "\n",
    "@dataclass\n",
    "class Args():\n",
    "    data_dir='/datasets/evidationdata'\n",
    "    split_seed=0\n",
    "    regularly_sampled=True\n",
    "    all_survey=False\n",
    "    min_date='2019-10-01' #None or format 'yyyy-mm-dd'\n",
    "    max_date='2020-08-14' #None or format 'yyyy-mm-dd'\n",
    "    # This argument is now required for the GET_PATH_DICT_KEY function, for the dataloader \n",
    "    # it should always be False the fake_data is made in a separate file given the correct base data.\n",
    "    fake_data=False\n",
    "    # if from_src then the data will be loaded from the original files from Raghu, if not from_src, \n",
    "    # then it will reload from the the final csv's if they exist.\n",
    "    from_src=True\n",
    "    # wave, is an integer indicating which set of data from Raghu to load from, currently they are just wave1 and wave2.\n",
    "    # the integers correspond to entries in a dictionary which contains the paths for a given wave in the function load_data.\n",
    "    wave=3\n",
    "    \n",
    "\n",
    "args=Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def merge_helper(sub_group):\n",
    "    \n",
    "    value = sub_group.aggregate({'date_onset_merged':'min', 'date_recovery_merged': 'max'})[['date_onset_merged', 'date_recovery_merged']]\n",
    "\n",
    "    res = sub_group\n",
    "    \n",
    "    res['new_date_onset_merged']=pd.to_datetime(value['date_onset_merged'])\n",
    "    res['new_date_recovery_merged']=pd.to_datetime(value['date_recovery_merged'])\n",
    "\n",
    "    return res\n",
    "    \n",
    "def merge_close(participant):\n",
    "    \n",
    "    test = (participant.groupby((participant['date_onset_merged'] - participant['date_recovery_merged'].shift() > pd.to_timedelta(7, unit='days')).cumsum())).apply(merge_helper)\n",
    "    \n",
    "    merge_close.num_calls += 1\n",
    "    if merge_close.num_calls % 1000 == 0:\n",
    "        print(merge_close.num_calls)\n",
    "    \n",
    "    return test\n",
    "merge_close.num_calls = 0\n",
    "\n",
    "def load_data(args):\n",
    "    \"\"\"\n",
    "    load the data from the data dir.\n",
    "    inputs:\n",
    "        args (argparse.ArgParser): arguments containing the data_dir\n",
    "    returns:\n",
    "        (dict): a dictionary of pandas.DataFrames\n",
    "    \"\"\"\n",
    "    wave_dict = {1: {'numeric': 'wave1/fbml_day_level_features.feather', \n",
    "                     'baseline': 'wave1/baseline_health_data.csv',\n",
    "                     'survey': 'wave1/survey_data.csv'},\n",
    "                 2: {'numeric': 'wave2/day_level_activity_features.feather', \n",
    "                     'baseline': 'wave2/demographic_health_data.csv',\n",
    "                     'survey': 'wave2/survey_data.csv'},\n",
    "                 3: {'numeric': 'wave2/day_level_activity_features.feather', \n",
    "                     'baseline': 'wave3/demographics.csv',\n",
    "                     'survey': 'wave3/survey.csv'},}\n",
    "    \n",
    "    dfs={}\n",
    "    \n",
    "    #numeric features\n",
    "    if os.path.exists(os.path.join(args.data_dir, 'activity_data.csv')) and not args.from_src:\n",
    "        out_df = pd.read_csv(os.path.join(args.data_dir, 'activity_data.csv'))\n",
    "    else:\n",
    "        out_df = pd.read_feather(os.path.join(args.data_dir, wave_dict[args.wave]['numeric']))\n",
    "        out_df['date'] = pd.to_datetime(out_df['date'])\n",
    "        print(out_df.columns.tolist())\n",
    "        if args.wave == 1:\n",
    "            out_df=out_df.loc[out_df['date']<= '2020-04-01']\n",
    "            out_df2 = pd.read_feather(os.path.join(args.data_dir, 'wave1/fbml_day_level_features-2020-04-01_to_2020-04-17.feather'))\n",
    "            out_df2['date'] = pd.to_datetime(out_df2['date'])\n",
    "            print(out_df2.columns.tolist())\n",
    "            out_df=out_df.append(out_df2, ignore_index=True).drop_duplicates(keep='last')\n",
    "    out_df = out_df.drop([s for s in out_df.columns.to_list() if 'sleep__main_start_time' in s ][0], axis = 1)\n",
    "    out_df['date'] = pd.to_datetime(out_df['date'])\n",
    "    if args.min_date is not None:\n",
    "        out_df=out_df.loc[out_df['date']>=args.min_date]\n",
    "    if args.max_date is not None:\n",
    "        out_df=out_df.loc[out_df['date']<= args.max_date]\n",
    "    out_df.set_index(['participant_id', 'date'], inplace=True)\n",
    "    out_df=out_df.groupby(['participant_id', 'date']).max()\n",
    "    # if all data in a row is 0, other than the two sleep features, then we should drop those data points.\n",
    "    zero_default_cols= ['sleep__sleep__total_asleep_minutes', 'sleep__sleep__total_in_bed_minutes']\n",
    "    # comment this out because we fixed the consent ranges for participants.\n",
    "#     print('before drop: ', len(out_df))\n",
    "    out_df[zero_default_cols] = out_df[zero_default_cols].replace({0:np.nan})\n",
    "#     out_df_no_missingness_index = out_df[QC_VARIABLES].dropna(how='all').index\n",
    "#     out_df = out_df.loc[out_df_no_missingness_index, :]\n",
    "#     print('after drop: ', len(out_df))\n",
    "    \n",
    "    display(out_df.head())\n",
    "    participant_set=out_df.index.get_level_values('participant_id')\n",
    "    dfs['activity'] = out_df.copy()\n",
    "    print('Done numeric features')\n",
    "    \n",
    "    #static features\n",
    "    if os.path.exists(os.path.join(args.data_dir, 'baseline_health_data.csv')) and not args.from_src:\n",
    "        out_df = pd.read_csv(os.path.join(args.data_dir, 'baseline_health_data.csv'), index_col='participant_id')\n",
    "    else: \n",
    "        out_df = pd.read_csv(os.path.join(args.data_dir, wave_dict[args.wave]['baseline']))\n",
    "        # the participant_id column is called user_id in wave2, change it for consistency.\n",
    "        out_df.columns = out_df.columns.str.replace('user_id', 'participant_id')\n",
    "        out_df = out_df.set_index('participant_id')\n",
    "    out_df=out_df.loc[list(set(participant_set).intersection(set(out_df.index.tolist()))),:]\n",
    "    out_df = one_hot_encode_cols(out_df, BASELINE_CATEGORICAL_COLUMNS)        \n",
    "    out_df[BASELINE_ZEROFILL] = out_df[BASELINE_ZEROFILL].fillna(0)\n",
    "    out_df[BASELINE_MEDIANFILL] = out_df[BASELINE_MEDIANFILL].fillna(out_df[BASELINE_MEDIANFILL].median())\n",
    "    out_df[BASELINE_COLS] = out_df[BASELINE_COLS].apply(pd.to_numeric)\n",
    "    dfs['baseline']=out_df[BASELINE_COLS+['state']].copy()\n",
    "    print('Done static features')\n",
    "    \n",
    "    \n",
    "    # labels_dataframe\n",
    "    if os.path.exists(os.path.join(args.data_dir, 'survey_data.csv')) and not args.from_src:\n",
    "        out_df = pd.read_csv(os.path.join(args.data_dir, 'survey_data.csv'))\n",
    "    else:\n",
    "        out_df = pd.read_csv(os.path.join(args.data_dir, wave_dict[args.wave]['survey']))\n",
    "    \n",
    "    out_df['date_survey'] = pd.to_datetime(out_df['date_survey'])\n",
    "    out_df['date_onset_merged'] = pd.to_datetime(out_df['date_onset_merged'])\n",
    "    out_df['date_recovery_merged'] = pd.to_datetime(out_df['date_recovery_merged'])\n",
    "    \n",
    "    out_df.set_index(['participant_id', 'date_survey'], inplace=True)\n",
    "    \n",
    "#     out_df=out_df.iloc[:50000] # set this for debug\n",
    "    \n",
    "    # Keep only survey data for participants with activity data as well.\n",
    "    if not(args.all_survey):\n",
    "        merged_index=list(set(participant_set).intersection(set(out_df.index.get_level_values('participant_id'))))\n",
    "        out_df=out_df.loc[out_df.index.get_level_values('participant_id').isin(merged_index),:]\n",
    "    \n",
    "    # Merge events where the previous date of recovery is only 7 days away from the next date of onset\n",
    "    out_df.reset_index(inplace=True)\n",
    "    out_df = out_df.groupby('participant_id').apply(merge_close) \n",
    "    out_df['date_onset_merged']=out_df['new_date_onset_merged'].values\n",
    "    out_df['date_recovery_merged']=out_df['new_date_recovery_merged'].values\n",
    "    \n",
    "    out_df.set_index(['participant_id', 'date_survey'], inplace=True)\n",
    "    \n",
    "    # create test set participants from the set of participants in the survey, this won't remove them yet, \n",
    "    # just not calculate their event dates.\n",
    "#     test_participants = out_df.groupby('participant_id').agg({'date_onset_merged': 'min'})\n",
    "#     test_participants = np.unique(test_participants[test_participants['date_onset_merged'] > '2020-06-29'].index.get_level_values('participant_id'))\n",
    "#     print('Number of participants in the test set: ' + str(len(test_participants)))\n",
    "#     train_participants = [p for p in np.unique(out_df.index.get_level_values('participant_id')) if p not in test_participants]\n",
    "#     out_df = out_df.loc[train_participants]\n",
    "#     dfs['train_participants'] = train_participants\n",
    "#     dfs['test_participants'] = test_participants\n",
    "    out_df.fillna(0, inplace=True)\n",
    "    \n",
    "    dfs['survey']= prepare_labels_df(out_df)\n",
    "    \n",
    "    # setting a min_date also helps to remove rows which are introduced because of far earlier health problems, for example:\n",
    "    # \n",
    "    if args.min_date is not None:\n",
    "        dfs['survey']=dfs['survey'].loc[dfs['survey'].index.get_level_values('date_survey')>=args.min_date]\n",
    "    if args.max_date is not None:\n",
    "        dfs['survey']=dfs['survey'].loc[dfs['survey'].index.get_level_values('date_survey')<=args.max_date]\n",
    "    print('Done labels')\n",
    "    \n",
    "#     print(set(dfs['survey'].index.tolist()))\n",
    "#     print(set(dfs['activity'].index.tolist()))\n",
    "\n",
    "    print('before', len(dfs['survey'].index), len(dfs['activity'].index), )\n",
    "    \n",
    "    #join the indices of everything\n",
    "    if not(args.all_survey):\n",
    "        dfs['survey'] = dfs['survey'].reindex(dfs['survey'].index.union(dfs['activity'].index).drop_duplicates())\n",
    "        dfs['survey'].index.names = ['participant_id', 'date']\n",
    "        dfs['activity'] = dfs['activity'].reindex(dfs['activity'].index.union(dfs['survey'].index).drop_duplicates())\n",
    "        dfs['activity'].index.names = ['participant_id', 'date']\n",
    "    else:\n",
    "        # activity should only be the activity where the survey is OK. (intersection, not union)\n",
    "        # survey index in activity participants\n",
    "        dfs['survey'].index.names = ['participant_id', 'date']\n",
    "        survey_activity_intersection = dfs['survey'].loc[dfs['survey'].index.get_level_values('participant_id').isin(set(dfs['activity'].index.get_level_values('participant_id')))].index\n",
    "        dfs['activity'] = dfs['activity'].reindex(dfs['activity'].index.union(survey_activity_intersection).drop_duplicates())\n",
    "        dfs['activity'].index.names = ['participant_id', 'date']\n",
    "    \n",
    "    print('after', len(dfs['survey'].index), len(dfs['activity'].index), )\n",
    "    \n",
    "    # Remove the participants from the activity and survey data so that only the training set of participants is in the data.\n",
    "#     dfs['survey'] = dfs['survey'].loc[train_participants]\n",
    "#     dfs['activity'] = dfs['activity'].loc[train_participants]\n",
    "    \n",
    "#     print(set(dfs['survey'].index.tolist()))\n",
    "#     print(set(dfs['activity'].index.tolist()))\n",
    "    \n",
    "    if args.regularly_sampled==True:\n",
    "        # for models that don't handle missingness.\n",
    "        min_inds= dfs['activity'].reset_index('date').groupby('participant_id').min().set_index(['date'], append=True).index\n",
    "        max_inds= dfs['activity'].reset_index('date').groupby('participant_id').max().set_index(['date'], append=True).index\n",
    "\n",
    "        person, min_i = zip(*sorted(min_inds.tolist()))\n",
    "        person, max_i = zip(*sorted(max_inds.tolist()))\n",
    "        all_inds=zip(person, min_i, max_i)\n",
    "\n",
    "        result_inds=[]\n",
    "        for ind in all_inds:\n",
    "            dates = pd.date_range(start=ind[1], end=ind[2])\n",
    "            result_inds+=list(zip([ind[0]]* len(dates), list(dates)))\n",
    "        result_inds = pd.MultiIndex.from_tuples(result_inds, names = ['participant_id', 'date'])        \n",
    "        dfs['survey'] = dfs['survey'].reindex(result_inds)\n",
    "        dfs['activity'] = dfs['activity'].reindex(result_inds)\n",
    "    \n",
    "    dfs['activity']['weekday']=(dfs['activity'].index.get_level_values('date').weekday<5).astype(np.int32) # add week of year\n",
    "\n",
    "    print('Done Participant Join')\n",
    "    \n",
    "    # now the labels must be filled in\n",
    "    \n",
    "    #get the index of the first date reported in the dataset for each participant\n",
    "    tmp = dfs['survey'].reset_index()[['participant_id', 'date']].groupby('participant_id').min()\n",
    "    tmp_index = tmp.set_index(['date'], append=True, drop=False).index\n",
    "    \n",
    "    #columns that are normally 0  \n",
    "    zero_start_cols=[col for col in dfs['survey'].columns if col not in EXCLUDE_ZERO_FILL_LABEL_COLS]\n",
    "    dfs['survey'].loc[tmp_index, zero_start_cols] = dfs['survey'].loc[tmp_index, zero_start_cols].fillna(0)\n",
    "    \n",
    "    # columns that are normally 1 (covid_no_sym)\n",
    "    one_start_cols = ['covid__symptoms__none', 'symptoms__no_symptoms', 'covid__behavior__social_distancing__did_not']\n",
    "    dfs['survey'].loc[tmp_index, one_start_cols] = dfs['survey'].loc[tmp_index, one_start_cols].fillna(1)\n",
    "    \n",
    "    # this tmp index is just for plotting df \n",
    "    tmp = dfs['survey'].reset_index()[['participant_id', 'date']].groupby('participant_id').apply(lambda x:x.nsmallest(2, 'date')).reset_index(drop=True)\n",
    "    tmp_index = tmp.set_index(['participant_id', 'date'], append=False, drop=False).index\n",
    "    tmp = dfs['survey'].reset_index()[['participant_id', 'date']].groupby('participant_id').max()\n",
    "    tmp_index = tmp_index.union(tmp.set_index(['date'], append=True, drop=False).index)\n",
    "    \n",
    "    display(dfs['survey'].loc[tmp_index, :])\n",
    "    \n",
    "    print(\"forward filling...\")\n",
    "    # impute the zero start and one-start cols by forward filling\n",
    "#     dfs['survey'].loc[:, zero_start_cols+one_start_cols] = dfs['survey'].loc[:, zero_start_cols+one_start_cols].bfill()\n",
    "    dfs['survey'].loc[:, zero_start_cols+one_start_cols] = dfs['survey'].loc[:, zero_start_cols+one_start_cols].ffill()\n",
    "    display(dfs['survey'].loc[tmp_index, :])\n",
    "    \n",
    "    if args.wave==2:\n",
    "        print('Enforcing limits')\n",
    "        limit_df = pd.read_csv(os.path.join(args.data_dir, 'wave2', 'activity_start_end_date.csv'))\n",
    "        dfs = apply_limits(dfs, limit_df) # apply limits is from run_model\n",
    "    \n",
    "    # backward filling won't fill the zeros for the healthy days of the last participant\n",
    "#     dfs['survey'][['ili', 'ili_24', 'ili_48', 'covid', 'covid_24', 'covid_48']] = dfs['survey'][['ili', 'ili_24', 'ili_48', 'covid', 'covid_24', 'covid_48']].fillna(0) \n",
    "#     display(dfs['survey'].loc[tmp_index, :])\n",
    "    \n",
    "    print(\"Done forward filling!\")\n",
    "    \n",
    "    # Make a two class label, 0=healthy, 1=flu 2=covid\n",
    "    # Commented out lines was a check for if there are overlapping medical__diagnosed and covid.\n",
    "#     flu_and_covid = dfs['survey']['medical__diagnosed'] + dfs['survey']['covid']\n",
    "#     assert len(flu_and_covid[flu_and_covid == 2]) == 0, 'found overlap of flu diagnosis and covid diagnosis labels'\n",
    "    # Sum the two together giving 1 if medical__diagnosed ili 2 if covid, 3 if both.\n",
    "    dfs['survey']['flu_covid'] = ((dfs['survey']['medical__diagnosed'] == 1)&(dfs['survey']['ili'] == 1)) + 2*dfs['survey']['covid'] + 2*dfs['survey']['covid']\n",
    "    # Since there is overlap between medical__diagnosed and covid, if the sum is greater than two set it to two \n",
    "    # indicating covid.\n",
    "    tmp = dfs['survey'][['flu_covid']]\n",
    "    tmp['two'] = 2\n",
    "    dfs['survey']['flu_covid'] = tmp.min(axis=1)\n",
    "        \n",
    "    return dfs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_labels_df(input_df):\n",
    "    \"\"\"\n",
    "    Convert *d_ago into timeseries\n",
    "    Convert complication onset into timeseries\n",
    "    inputs:\n",
    "        input_df (pd.DataFrame)\n",
    "    returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    label_df = input_df.copy()\n",
    "\n",
    "#     label_df.columns = [col if '0d_ago' not in col else col.replace('__0d_ago', '') for col in label_df.columns]\n",
    "    for col in label_df.columns:\n",
    "        if '0d_ago' in col:\n",
    "            print(col)\n",
    "            if col.replace('__0d_ago', '') not in label_df.columns:\n",
    "                label_df[col.replace('__0d_ago', '')] = label_df[col]\n",
    "    \n",
    "    label_df['time_to_survey']=0\n",
    "    \n",
    "    label_df = label_df.loc[:, ~label_df.columns.duplicated()]\n",
    "    \n",
    "    # reconcile multilabel targets\n",
    "    \n",
    "    # create covid_cohort category.\n",
    "    idx=pd.IndexSlice\n",
    "    label_df.loc[:, 'covid_cohort']=0\n",
    "    label_df['covid__diagnosed'] = label_df['covid__diagnosed'].str.lower()\n",
    "    covid_tested = set(label_df.loc[label_df['covid__diagnosed'].isin(['yes', 'no', 'i am waiting for my diagnosis']), :].index.get_level_values('participant_id'))\n",
    "#     print('yes: ', len(set(label_df.loc[label_df['covid__diagnosed'].isin(['yes']), :].index.get_level_values('participant_id'))))\n",
    "#     print('no: ', len(set(label_df.loc[label_df['covid__diagnosed'].isin(['no']), :].index.get_level_values('participant_id'))))\n",
    "#     print('waiting: ',len(set(label_df.loc[label_df['covid__diagnosed'].isin(['i am waiting for my diagnosis']), :].index.get_level_values('participant_id'))))\n",
    "\n",
    "    label_df.loc[idx[covid_tested, :], 'covid_cohort']=1    \n",
    "    \n",
    "    #assert len(covid_tested)<=177+1324+192 # from the survey data as of May 13\n",
    "    #pd.get_dummies one hot encode covid_diagnosed with no nan col\n",
    "    label_df=pd.get_dummies(label_df, columns=['covid__diagnosed']\n",
    "                           ).join(label_df['covid__diagnosed'])\n",
    "    label_df['covid__diagnosed'] = label_df['covid__diagnosed'].replace({'yes':1, 'no': 0, 'i am waiting for my diagnosis':np.nan,  'i don t know i can t remember':np.nan})\n",
    "    label_df['covid__diagnosed'] = label_df['covid__diagnosed'].apply(lambda x: np.nan if isinstance(x, str) else x)\n",
    "    label_df['covid_diagnosed_date'] = label_df['covid__diagnosed'].fillna(np.nan)\n",
    "    \n",
    "    med_diag = label_df['medical__diagnosed'].astype(str).str.lower()\n",
    "    med_responses = np.unique(med_diag)\n",
    "    replacement_dict = {}\n",
    "    for r in med_responses:\n",
    "        replacement_dict[r] = 1 if r == 'yes' else 0\n",
    "    label_df['medical__diagnosed'] = med_diag.replace(replacement_dict)\n",
    "#     label_df.loc[~label_df['medical__diagnosed'].isna(), 'medical__diagnosed']=(label_df.loc[~label_df['medical__diagnosed'].isna(), 'medical__diagnosed']=='yes').astype(np.int32)\n",
    "    \n",
    "        \n",
    "        \n",
    "    # clean up the categories that don't binarise nicely\n",
    "    label_df['home__household_members_had_flu'] = pd.to_numeric(label_df['home__household_members_had_flu'].str.lower().replace({'yes':1}), errors='coerce').fillna(0)\n",
    "    label_df['home__household_members_n'] = pd.to_numeric(label_df['home__household_members_n'].str.lower().replace({'>10':11}), errors='coerce').fillna(0)\n",
    "    label_df['medical__hospitalized'] = pd.to_numeric(label_df['medical__hospitalized'].str.lower().replace({'yes':1, 'no':0}), errors='coerce').fillna(0)\n",
    "    label_df['medical__medication'] = pd.to_numeric(label_df['medical__medication'].str.lower().replace({'yes':1, 'no':0}), errors='coerce').fillna(0)\n",
    "    label_df['medical__medication_otc'] = pd.to_numeric(label_df['medical__medication_otc'].str.lower().replace({'yes':1, 'no':0}), errors='coerce').fillna(0)\n",
    "    label_df['medical__sought_attention'] = pd.to_numeric(label_df['medical__sought_attention'].str.lower().replace({'yes':1, 'no':0}), errors='coerce').fillna(0)\n",
    "    label_df['covid__quarantine'] = pd.to_numeric(label_df['covid__quarantine'].str.lower().replace({'yes':1, 'no':0}), errors='coerce').fillna(0)\n",
    "    label_df['medical__vaccinated_last_year'] = pd.to_numeric(label_df['medical__vaccinated_last_year'].str.lower().replace({'yes':1, 'no':0}), errors='coerce').fillna(0)\n",
    "    label_df['medical__vaccinated_this_year'] = pd.to_numeric(label_df['medical__vaccinated_this_year'].str.lower().replace({'yes':1, 'no':0}), errors='coerce').fillna(0)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # replace strings with binary indicators\n",
    "    print(\"BEFORE:\")\n",
    "    print(label_df.groupby('covid__any_household_diagnosed').count())\n",
    "    BINARY_COLS2=[col for col in BINARY_COLS if col!='covid__diagnosed']\n",
    "    label_df[BINARY_COLS2] = label_df[BINARY_COLS2].notnull().astype('int')    \n",
    "    print(\"AFTER:\")\n",
    "    print(label_df.groupby('covid__any_household_diagnosed').count())\n",
    "    \n",
    "    \n",
    "    # expand index\n",
    "    all_inds = label_df.index.tolist()\n",
    "    for offset in range(0, 7):\n",
    "        # fill in dates for the missing days of the week\n",
    "        additional_index = [(item[0], item[1]-pd.to_timedelta(offset, unit='d')) for item in  set(input_df.index.tolist())]\n",
    "        original_inds=deepcopy(all_inds)\n",
    "        all_inds += additional_index\n",
    "        \n",
    "        new_index = pd.MultiIndex.from_tuples(list(set(all_inds)), names=label_df.index.names).drop_duplicates()\n",
    "        label_df = label_df.reindex(new_index)\n",
    "        label_df['time_to_survey'] = label_df['time_to_survey'].fillna(offset) # add the time to survey here for reliability's sake\n",
    "\n",
    "        label_df = label_df.sort_index()\n",
    "        \n",
    "        # bfill columns that have {offset}d_ago in them\n",
    "        bfill_columns = [col for col in label_df.columns if f\"__{offset}d_ago\" in col] # this is why I go one row at a time\n",
    "        \n",
    "        label_df[bfill_columns]=label_df[bfill_columns].fillna(method='bfill', limit=1)\n",
    "\n",
    "\n",
    "        # fill data with a dict of series using the *d_ago values\n",
    "        replace_vals={col.replace(f\"__{offset}d_ago\", \"\"):label_df[col] for col in label_df.columns if f\"__{offset}d_ago\" in col}\n",
    "        label_df = label_df.fillna(replace_vals)\n",
    "        \n",
    "    \n",
    "        \n",
    "    # now convert the complication columns to timeseries by adding them to the index and one-hot encoding\n",
    "    \n",
    "    for col in label_df.columns:\n",
    "        if '__onset' not in col:\n",
    "            continue\n",
    "        label_df[col]= pd.to_datetime(label_df[col], errors='coerce')\n",
    "        tups=list(zip(label_df.loc[~label_df[col].isna(), :].index.get_level_values('participant_id'),label_df.loc[~label_df[col].isna(), col].values))\n",
    "        \n",
    "        # join these new dates into the index.\n",
    "        new_index = pd.MultiIndex.from_tuples(list(set(label_df.index.tolist()+tups)), names=label_df.index.names).drop_duplicates()\n",
    "        label_df = label_df.reindex(new_index)\n",
    "        \n",
    "        label_df.loc[:, col.replace('__onset', '')]=0\n",
    "        label_df.loc[tups, col.replace('__onset', '')]=1\n",
    "        \n",
    "        \n",
    "    # now convert the date columns to timeseries by adding them to the index and one-hot encoding\n",
    "    \"\"\"\n",
    "    ILI Labels\n",
    "    we will follow the following procedure:\n",
    "    1) Get the day after date_recovery_merged\n",
    "    2) Set these values to 0 for ILI\n",
    "    3) Get the index of date_onset_merged\n",
    "    4) Set these values to 1 for ILI\n",
    "    \"\"\"\n",
    "    label_df.loc[:, 'ili']=np.nan\n",
    "    label_df.loc[:, 'ili_24']=np.nan\n",
    "    label_df.loc[:, 'ili_48']=np.nan\n",
    "    \n",
    "    \n",
    "    # **************************\n",
    "    # 1)\n",
    "    \n",
    "    col = 'date_recovery_merged'\n",
    "    label_df[col]= pd.to_datetime(label_df[col], errors='coerce')+pd.to_timedelta(1, unit='D')\n",
    "    tups=list(zip(label_df.loc[~label_df[col].isna(), :].index.get_level_values('participant_id'),label_df.loc[~label_df[col].isna(), col].values))\n",
    "    col_mod='date_onset_merged' # this will be 0 for ili_24 and ili_48\n",
    "    tups_24=list(zip(label_df.loc[~label_df[col].isna(), :].index.get_level_values('participant_id'),label_df.loc[~label_df[col].isna(), col_mod].values))\n",
    "    tups_48=list(zip(label_df.loc[~label_df[col].isna(), :].index.get_level_values('participant_id'),label_df.loc[~label_df[col].isna(), col_mod].values))\n",
    "\n",
    "    # join these new dates into the index.\n",
    "    new_index = pd.MultiIndex.from_tuples(list(set(label_df.index.tolist()+tups+tups_24+tups_48)), names=label_df.index.names).drop_duplicates()\n",
    "    label_df = label_df.reindex(new_index)\n",
    "    \n",
    "    # **************************\n",
    "    # 2) the end of each label is being set to zero\n",
    "\n",
    "    label_df.loc[tups, 'ili']=0 if 'recovery' in col else 1\n",
    "    label_df.loc[tups_24, 'ili_24']=0 if 'recovery' in col else 1\n",
    "    label_df.loc[tups_48, 'ili_48']=0 if 'recovery' in col else 1\n",
    "    \n",
    "    # **************************\n",
    "    # 3)\n",
    "    \n",
    "    col = 'date_onset_merged'\n",
    "    label_df[col]= pd.to_datetime(label_df[col], errors='coerce')\n",
    "    tups=list(zip(label_df.loc[~label_df[col].isna(), :].index.get_level_values('participant_id'),label_df.loc[~label_df[col].isna(), col].values))\n",
    "    tups_24=list(zip(label_df.loc[~label_df[col].isna(), :].index.get_level_values('participant_id'),(label_df.loc[~label_df[col].isna(), col]-pd.to_timedelta(1, unit='D')).values))\n",
    "    tups_48=list(zip(label_df.loc[~label_df[col].isna(), :].index.get_level_values('participant_id'),(label_df.loc[~label_df[col].isna(), col]-pd.to_timedelta(2, unit='D')).values))\n",
    "\n",
    "    new_index = pd.MultiIndex.from_tuples(list(set(label_df.index.tolist()+tups+tups_24+tups_48)), names=label_df.index.names).drop_duplicates()\n",
    "    label_df = label_df.reindex(new_index)\n",
    "    \n",
    "    # **************************\n",
    "    # 4)\n",
    "    # set start of 1 labels        \n",
    "    label_df.loc[tups, 'ili']=0 if 'recovery' in col else 1\n",
    "    label_df.loc[tups_24, 'ili_24']=0 if 'recovery' in col else 1\n",
    "    label_df.loc[tups_48, 'ili_48']=0 if 'recovery' in col else 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    COVID Labels\n",
    "    we will follow the following procedure:\n",
    "    1) Get the day after date_recovery_merged\n",
    "    2) Set these values to 0 for covid__diagnosed\n",
    "    3) Get the index of date_onset_merged\n",
    "    4) Set these values to 1 for covid__diagnosed\n",
    "    5) Make the default starting value for covid__diagnosed 0\n",
    "    \"\"\"\n",
    "        \n",
    "    label_df['covid_range']=np.nan\n",
    "    \n",
    "    # **************************\n",
    "    # 1)    \n",
    "    # recall we shifted the recovey date in the ili labels\n",
    "    covid_recovery_tups = list(zip(label_df.loc[(~label_df['date_recovery_merged'].isna())&(label_df['covid__diagnosed']==1), :].index.get_level_values('participant_id'),\n",
    "                                   label_df.loc[(~label_df['date_recovery_merged'].isna())&(label_df['covid__diagnosed']==1), 'date_recovery_merged'].values))\n",
    "    # **************************\n",
    "    # 2)\n",
    "    label_df.loc[covid_recovery_tups, 'covid_range']=0 # we don't need to reindex because all date recoveries are already done\n",
    "    # **************************\n",
    "    # 3)\n",
    "    covid_onset_tups = list(zip(label_df.loc[(~label_df['date_onset_merged'].isna())&(label_df['covid__diagnosed']==1), :].index.get_level_values('participant_id'),\n",
    "                                (label_df.loc[(~label_df['date_onset_merged'].isna())&(label_df['covid__diagnosed']==1), 'date_onset_merged']-pd.to_timedelta(2, unit='D')).values))\n",
    "    # **************************\n",
    "    # 4)\n",
    "    label_df.loc[covid_onset_tups, 'covid_range']=1 # we don't need to reindex because all date recoveries are already done\n",
    "    # **************************\n",
    "    # 5)\n",
    "    # set the min index for each participant equal to 0\n",
    "    label_df = label_df.sort_index()\n",
    "    tmp = label_df.reset_index()[['participant_id', 'date_survey']].groupby('participant_id').min()\n",
    "    tmp_index = tmp.set_index(['date_survey'], append=True, drop=False).index\n",
    "    label_df.loc[tmp_index, 'covid_range'] = label_df.loc[tmp_index, 'covid_range'].fillna(0)\n",
    "    \n",
    "    # **************************\n",
    "    # 6) groupby and forward fill \n",
    "    label_df['covid_range'] = label_df['covid_range'].ffill()\n",
    "    # step 5 is unecessary if we just did the following (but groupbys are slow)\n",
    "    label_df['covid_range'] = label_df['covid_range'].groupby('participant_id').ffill() # this could potentially be redundant.\n",
    "    label_df['covid_range']=label_df['covid_range'].fillna(0)\n",
    "    \n",
    "    # **************************\n",
    "    # 7)\n",
    "    # multiply the covid__diagnosed colum by the ili column to get the covid labels\n",
    "\n",
    "    label_df['covid']=np.nan\n",
    "    label_df['covid_24']=np.nan\n",
    "    label_df['covid_48']=np.nan\n",
    "        \n",
    "    # match up with ILI\n",
    "    label_df.loc[(label_df['ili']==0), 'covid']=0 # this expicitly excludes participants without symptoms. (they do not exist in the dataset)\n",
    "    label_df.loc[(label_df['ili_24']==0), 'covid_24']=0\n",
    "    label_df.loc[(label_df['ili_48']==0), 'covid_48']=0\n",
    "    \n",
    "    label_df.loc[(label_df['covid_range']==1)&(label_df['ili']==1), 'covid']=1 # this expicitly excludes participants without symptoms. (they do not exist in the dataset)\n",
    "    label_df.loc[(label_df['covid_range']==1)&(label_df['ili_24']==1), 'covid_24']=1\n",
    "    label_df.loc[(label_df['covid_range']==1)&(label_df['ili_48']==1), 'covid_48']=1\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Now handle the categorical variables:\n",
    "    for feat in ['covid__behavior__air_travel', 'covid__contact_ILI_outside_household', 'covid__contact_covid']:\n",
    "        # get additional indices\n",
    "        print(set(input_df[feat].values), input_df[feat].fillna('no').astype(str).str.contains('7 days').sum())\n",
    "        \n",
    "        index_for_feature = input_df.loc[input_df[feat].fillna('no').astype(str).str.contains('7 days')].index.tolist()\n",
    "        \n",
    "        result_index = index_for_feature\n",
    "        # for last_7 days\n",
    "        for offset in range(1, 7):\n",
    "            result_index += [(item[0], item[1]-pd.to_timedelta(offset, unit='d')) for item in set(index_for_feature)]\n",
    "        \n",
    "        index_for_feature = input_df.loc[input_df[feat].fillna('no').astype(str).str.contains('last 14 days')].index.tolist()\n",
    "        # for last 14 days\n",
    "        for offset in range(7, 14):\n",
    "            result_index += [(item[0], item[1]-pd.to_timedelta(offset, unit='d')) for item in set(index_for_feature)]\n",
    "        \n",
    "        all_inds = deepcopy(label_df.index)\n",
    "        print(feat, len(set(result_index)-set(all_inds.tolist())))\n",
    "        \n",
    "        new_index = pd.MultiIndex.from_tuples(list(set(result_index)), names=label_df.index.names).drop_duplicates()\n",
    "        \n",
    "        \n",
    "        label_df[feat]=0 # first set everything to 0\n",
    "        label_df.loc[all_inds.intersection(new_index), feat]=1 # now set the intersecting labels to 1\n",
    "    \n",
    "\n",
    "\n",
    "    for col in SURVEY_ROLLING_FEATURES:\n",
    "        if col in ['covid__behavior__air_travel', 'covid__contact_ILI_outside_household', 'covid__contact_covid']:\n",
    "            continue\n",
    "#         print()\n",
    "#         print(col)\n",
    "#         print(label_df[col].dtype)\n",
    "        if label_df[col].dtype!=object:\n",
    "            continue\n",
    "        set(label_df[col].str.lower().values.tolist())\n",
    "        if set(label_df[col].str.lower().values.tolist())==set([np.nan, 'no', 'yes']):\n",
    "            label_df.loc[label_df[col].str.lower()=='no', col]=0\n",
    "            label_df.loc[label_df[col].str.lower()=='yes', col]=1\n",
    "            if col not in ['covid__symptoms__none', 'symptoms__no_symptoms', 'covid__behavior__social_distancing__did_not']:\n",
    "                label_df[col]=label_df[col].fillna(0) # this assumes no contact\n",
    "\n",
    "#         all_inds+= result_index\n",
    "#         new_index = pd.MultiIndex.from_tuples(list(set(all_inds)), names=label_df.index.names).drop_duplicates()\n",
    "#         label_df=label_df.reindex(new_index)\n",
    "        \n",
    "#         label_df[feat]=0\n",
    "#         label_df.loc[result_index, feat]=1\n",
    "\n",
    "    \n",
    "    return label_df[LABEL_COLS]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def one_hot_encode_cols(in_df, cols):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        in_df (pd.DataFrame): a dataframe with categorical features as strings\n",
    "        cols (list): the columns to one-hot-encode\n",
    "    returns:\n",
    "        pd.DataFrame: a dataframe with all columns categorically encoded\n",
    "    \"\"\"\n",
    "    df=in_df.copy()\n",
    "    for col in cols:\n",
    "        new_cols = pd.get_dummies(df[col], prefix=col)\n",
    "        df = pd.concat((df, new_cols), axis=1)\n",
    "        df=df.drop(col, axis=1)\n",
    "    return df\n",
    "    \n",
    "\n",
    "dfs=load_data(args)\n",
    "\n",
    "#todo must pass the following checks:\n",
    "if not(args.all_survey):\n",
    "    assert dfs['survey'].index.equals(dfs['activity'].index), \"The indices are not the same\"\n",
    "\n",
    "# check that all of our necessary outcomes are loaded\n",
    "assert \"ili\" in dfs['survey'].columns\n",
    "assert \"ili_24\" in dfs['survey'].columns\n",
    "assert \"ili_48\" in dfs['survey'].columns\n",
    "\n",
    "assert \"covid\" in dfs['survey'].columns\n",
    "assert \"covid_24\" in dfs['survey'].columns\n",
    "assert \"covid_48\" in dfs['survey'].columns\n",
    "\n",
    "# count number of covid\n",
    "print('number of ili participants: ',dfs['survey']['ili'].groupby('participant_id').max().sum() )\n",
    "print('number of covid participants: ',dfs['survey']['covid'].groupby('participant_id').max().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to hdf\n",
    "fname = 'all_daily_data.hdf'\n",
    "fname = 'all_daily_data_' + 'allsurvey_'*args.all_survey + 'regular'*args.regularly_sampled +'irregular'*(not(args.regularly_sampled)) + '_merged_apr27.hdf'\n",
    "\n",
    "path = os.path.join(args.data_dir, 'test', fname)\n",
    "\n",
    "dfs['baseline'].to_hdf(path, 'baseline', format='table')\n",
    "dfs['activity'].to_hdf(path, 'activity_raw')\n",
    "dfs['survey'].to_hdf(path, 'survey')\n",
    "\n",
    "# Store the path for the most recent run of this notebook so that each run \n",
    "# of this will update the path to be used for a given argument configuration.\n",
    "if os.path.exists(DATA_PATH_DICTIONARY_FILE):\n",
    "    with open(DATA_PATH_DICTIONARY_FILE, 'rb') as f:\n",
    "        path_dict = pickle.load(f)\n",
    "else:\n",
    "    path_dict = {}\n",
    "\n",
    "key = GET_PATH_DICT_KEY(args)\n",
    "\n",
    "path_dict[key] = path\n",
    "\n",
    "with open(DATA_PATH_DICTIONARY_FILE, 'wb') as f:\n",
    "    pickle.dump(path_dict, f)\n",
    "\n",
    "# pd.Series(dfs['train_participants']).to_hdf(os.path.join(args.data_dir, 'test', fname), 'train_participants')\n",
    "# pd.Series(dfs['test_participants']).to_hdf(os.path.join(args.data_dir, 'test', fname), 'test_participants')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load non-imputed dataframe from hdf\n",
    "# fname = 'all_daily_data.hdf'\n",
    "fname = 'all_daily_data_' + 'regular'*args.regularly_sampled +'irregular'*(not(args.regularly_sampled)) + '_merged3.hdf'\n",
    "fname = 'all_daily_data_' + 'allsurvey_'*args.all_survey + 'regular'*args.regularly_sampled +'irregular'*(not(args.regularly_sampled)) + '_merged3.hdf'\n",
    "fname = 'all_daily_data_' + 'allsurvey_'*args.all_survey + 'regular'*args.regularly_sampled +'irregular'*(not(args.regularly_sampled)) + '_merged_apr27.hdf'\n",
    "\n",
    "\n",
    "dfs = {}\n",
    "dfs['baseline'] = pd.read_hdf(os.path.join(args.data_dir, 'test', fname), 'baseline')\n",
    "dfs['activity'] = pd.read_hdf(os.path.join(args.data_dir, 'test', fname), 'activity_raw')\n",
    "dfs['survey'] = pd.read_hdf(os.path.join(args.data_dir, 'test', fname), 'survey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Imputation </h2>\n",
    "<a id=\"imputation\">Simple imputation for GRUD</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get means from healthy cohort\n",
    "inds = dfs['survey'].loc[dfs['survey']['ili_48']==0, :].index.tolist()\n",
    "\n",
    "train_means = dfs['activity'].loc[inds, :].sample(5000).mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def simple_impute(df_input, train_means, ID_COLS=['participant_id'], tqdm=tqdm):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    df_in=df_input.copy()\n",
    "\n",
    "    #masked data\n",
    "    masked_df=pd.notna(df_in)\n",
    "    masked_df=masked_df.apply(pd.to_numeric)\n",
    "\n",
    "    #time since last measurement\n",
    "    is_absent = (1 - masked_df).apply(pd.to_numeric)\n",
    "    date_diff = np.concatenate(([0], np.diff(is_absent.index.get_level_values('date'), n=1, axis=0) / np.timedelta64(1,'D')))\n",
    "    \n",
    "    \n",
    "    def cumsum_missing(xs):\n",
    "#         print(xs)\n",
    "        for col in xs.columns:\n",
    "            xs_out=[]\n",
    "            for i, x in enumerate(xs[col]):\n",
    "                if i==0:\n",
    "                    xs_out.append(0)\n",
    "                elif xs[col].values[i-1]==0:\n",
    "                    # data is not absent\n",
    "                    xs_out.append(date_diff[i])\n",
    "                else:\n",
    "                    xs_out.append(date_diff[i]+xs_out[i-1])\n",
    "            xs.loc[:, col]=xs_out\n",
    "        return xs\n",
    "    tqdm_pandas(tqdm())\n",
    "    time_df=is_absent.groupby(ID_COLS).progress_apply(cumsum_missing)\n",
    "    \n",
    "    is_absent.loc[:, :] = is_absent.values * date_diff[:, np.newaxis]\n",
    "                \n",
    "    \n",
    "    \n",
    "#     # if current mask is 1, then date_diff else cumsum\n",
    "#     hours_of_absence = is_absent.groupby(ID_COLS).cumsum()\n",
    "#     time_since_measured = hours_of_absence - hours_of_absence[is_absent==0].fillna(method='ffill')\n",
    "#     time_df = time_since_measured.fillna(0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #last observed value\n",
    "    X_last_obsv_df=df_input.copy()\n",
    "\n",
    "    # do the mean imputation for only the first hour\n",
    "    columns=X_last_obsv_df.columns.tolist()\n",
    "    \n",
    "    #Only do means where the column isn't the outcome\n",
    "    inds = X_last_obsv_df.reset_index('date').groupby('participant_id').min().set_index(['date'], append=True).index\n",
    "    subset_data=X_last_obsv_df.loc[inds, columns]\n",
    "\n",
    "    # (not sure how original paper did it, possibly just fill with zeros???)\n",
    "    subset_data=subset_data.fillna(train_means)\n",
    "\n",
    "    #replace first hour data with the imputed first hour data\n",
    "    X_last_obsv_df.loc[inds, columns] = subset_data.values\n",
    "\n",
    "    # now it is safe for forward fill\n",
    "    #forward fill the rest of the sorted data\n",
    "    X_last_obsv_df=X_last_obsv_df.fillna(method='ffill')\n",
    "    X_last_obsv_df=X_last_obsv_df.fillna(0)\n",
    "    \n",
    "    # todo: now do the normalised value with the offset\n",
    "    ffilled_norm_df=df_in.copy()\n",
    "    num_days_history = 28 # number of days to include in the history\n",
    "    num_days_minimum = 14\n",
    "    norm_by_past_days = 7\n",
    "    \n",
    "    # add gaussian noise of std=0.1 for hr \n",
    "    ffilled_norm_df = ffilled_norm_df.transform(lambda x:(x-x.rolling(num_days_history, num_days_minimum).mean().shift(norm_by_past_days))\\\n",
    "                                                            /x.rolling(num_days_history, num_days_minimum).std(ddof=0).shift(norm_by_past_days))\n",
    "    #six sigma\n",
    "#     lambda x: x if std(x)*6<abs(x) else np.nan\n",
    "    ffilled_norm_df_std = df_in.copy().transform(lambda x:x.rolling(num_days_history, num_days_minimum).std(ddof=0).shift(norm_by_past_days))\n",
    "    for col in ffilled_norm_df.columns:\n",
    "        ffilled_norm_df.loc[ffilled_norm_df[col].abs()>6*ffilled_norm_df_std[col], col]=np.nan\n",
    "    \n",
    "    ffilled_norm_df = ffilled_norm_df.groupby('participant_id').fillna(method='ffill')\n",
    "    norm_df = ffilled_norm_df.copy()\n",
    "    ffilled_norm_df = ffilled_norm_df.fillna(0)\n",
    "    \n",
    "    display(ffilled_norm_df.head(10))\n",
    "    \n",
    "       \n",
    "    \n",
    "    return df_in, norm_df, ffilled_norm_df, X_last_obsv_df, masked_df, time_df\n",
    "\n",
    "print('imputing')\n",
    "inp, norm_df, ffilled_norm_df, ffilled_df, mask_df, time_df = simple_impute(dfs['activity'], train_means)\n",
    "\n",
    "\n",
    "\n",
    "# ffilled_df.columns = pd.MultiIndex.from_product([ffilled_df.columns, ['measurement']])\n",
    "# mask_df.columns = pd.MultiIndex.from_product([mask_df.columns, ['mask']])\n",
    "# time_df.columns = pd.MultiIndex.from_product([time_df.columns, ['time']])\n",
    "\n",
    "df2 = pd.concat({'measurement_z':ffilled_norm_df.loc[ffilled_df.index, :], 'measurement':ffilled_df.loc[ffilled_df.index, :], 'measurement_noimp':inp, 'measurement_z_noimp': norm_df, 'mask':mask_df.loc[ffilled_df.index, :], 'time':time_df.loc[ffilled_df.index, :]}, axis=1, names=['df_type', 'value'])\n",
    "\n",
    "\n",
    "display(df2)\n",
    "\n",
    "df2.to_hdf(os.path.join(args.data_dir, 'test', fname), 'activity')\n",
    "\n",
    "print('done saving')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add time_to_onset to survey dataframe #\n",
    "\n",
    "<a id=\"time_to_onset\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe from hdf, to add time_to_onset to.\n",
    "# fname = 'all_daily_data.hdf'\n",
    "fname = 'all_daily_data_' + 'allsurvey_'*args.all_survey + 'regular'*args.regularly_sampled +'irregular'*(not(args.regularly_sampled)) + '.hdf'\n",
    "\n",
    "\n",
    "# Only need the survey dataframe to calculate this.\n",
    "dfs = {}\n",
    "dfs['survey'] = pd.read_hdf(os.path.join(args.data_dir, 'test', fname), 'survey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time_to_onset, add the column to the survey dataframe, save the new survey dataframe to the hdf file read from.\n",
    "def calculate_tto(group_df):\n",
    "    \"\"\"\n",
    "    Calculate time_to_onset series, time_to_onset is always the number of days closest to a date of onset for ILI, \n",
    "    negative indicates the date is before the nearest onset date. \n",
    "    \n",
    "    group_df: pandas grouped dataframe, grouped by the participant_id\n",
    "    \"\"\"\n",
    "    assert 'ili' in group_df.columns, \"Missing ili column in grouped dataframe to calculate time_to_onset\"\n",
    "    didx = pd.Series(group_df.index.get_level_values(1))\n",
    "    \n",
    "    ili_diff = group_df['ili'].droplevel(0).reindex(pd.DatetimeIndex([didx.min() - pd.Timedelta('1d')] + didx.to_list() + [didx.max() + pd.Timedelta('1d')]), fill_value=0).diff()\n",
    "    #drop the added dates to fix end point corner case for diff\n",
    "    ili_diff = ili_diff[1:-1]\n",
    "    \n",
    "    onset_dates = group_df.index.get_level_values('date')[ili_diff == 1]\n",
    "    \n",
    "    if len(onset_dates) == 0:\n",
    "        group_df['time_to_onset'] = np.nan\n",
    "        return group_df\n",
    "\n",
    "    onset_deltas = []\n",
    "    for d in onset_dates:\n",
    "        onset_deltas.append(pd.Series(group_df.index.get_level_values('date') - pd.to_datetime(d)))\n",
    "    onset_deltas = pd.concat(onset_deltas, axis=1)\n",
    "    \n",
    "    # If ili is negative time_to_onset is the closest onset in either direction\n",
    "    neg_time_to_onset = onset_deltas.apply(lambda x: x[np.argmin(x.abs())], axis=1).dt.days\n",
    "    neg_time_to_onset.index = group_df.index\n",
    "    \n",
    "    # time_to_onset is the closest onset prior to the current date.\n",
    "    max_delta = group_df.index.get_level_values('date').max() - group_df.index.get_level_values('date').min() + pd.to_timedelta(1, 'd')\n",
    "    positive_time_to_onset = onset_deltas.copy()\n",
    "    positive_time_to_onset[positive_time_to_onset < pd.to_timedelta(0, 'd')] = max_delta\n",
    "    positive_time_to_onset = positive_time_to_onset.min(axis=1).dt.days\n",
    "    positive_time_to_onset.index = group_df.index\n",
    "    \n",
    "    # if ili is positive use the backwards looking time_to_onset\n",
    "    pos_idxs = group_df['ili'] == 1\n",
    "    group_df.loc[pos_idxs, 'time_to_onset'] = positive_time_to_onset[pos_idxs.values]\n",
    "    \n",
    "    # otherwise use the closest time_to_onset\n",
    "    neg_idxs = group_df['ili'] == 0\n",
    "    group_df.loc[neg_idxs, 'time_to_onset'] = neg_time_to_onset[neg_idxs.values]\n",
    "    \n",
    "    return group_df\n",
    "\n",
    "\n",
    "# assert not 'time_to_onset' in dfs['survey'].columns, 'time_to_onset already present in this survey, may not need to recalculate.'\n",
    "\n",
    "tqdm().pandas()\n",
    "\n",
    "dfs['survey'] = dfs['survey'].groupby('participant_id').progress_apply(calculate_tto)\n",
    "dfs['survey'].to_hdf(os.path.join(args.data_dir, 'test', fname), 'survey')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
